{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "polyRegBachelier.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNVRmCRli4b9tX01Qxg1tud",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/differential-machine-learning/notebooks/blob/master/polyRegBachelier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk92RcHiLru-",
        "colab_type": "text"
      },
      "source": [
        "# Differential Regression\n",
        "\n",
        "---\n",
        "Antoine Savine, April 2020\n",
        "---\n",
        "\n",
        "\n",
        "This notebook applies differential learning in the context of classic regression models. It is meant as a complement to the article 'Learning the Shape: Differential Machine Learning' by Huge and Savine (Risk, 2020), which explored the new ideas of twin networks and differential training, mainly in the context of deep neural networks. The article hinted that the methodology applies to arbitrary regression models, including classic linear regression and neural architectures of arbitrary complexity, but did not show other numerical results than feedforward networks, leaving extensions for online additional material. \n",
        "\n",
        "This notebook substantiates the claim and applies the methodology to polynomial regression in the context of a basket option in a correlated Bachelier model, the same context as the first numerical example in the article. We see that, in this context too, differential training provides a  massive performance improvement, compared, not only with standard regression, but also, with ridge (Tikhonov) regularization, optimized by cross-validation. Optimized ridge regression consumes significant additional data and computation load, and it definitely improves the performance of the standard regression. Differential regression, however, performs orders of magnitude better, and without the need for additional regularization, or hyperparameter optimization.\n",
        "\n",
        "Although the notebook implements polynomial regression over Bachelier basket data, it is easy to change the simulation model, financial instrument and regression basis, for example to implement an Asian option in Black and Scholes with a radial basis regression... More complicated, real-world models and instruments cannot be directly implemented in numpy, for that we need a full financial Derivatives library in C++ or TensorFlow.\n",
        "\n",
        "Textbook regression is often implemented analytically with normal equations, or numerically with gradient descent. In low to medium dimension (1 to less than 10 in practice), the best results are obtained with SVD regression, which stabilises the analytic normal equations by removing insignificant singular values / eigenvalues. The notebook implements the formulas found in [this memo](https://drive.google.com/file/d/1qoZB_p9DeK-LZPV76lTkx7esduwme4FT/view?usp=sharing), based on eigenvalue decomposition.\n",
        "\n",
        "In the context of the polynomial regression over Bachelier basket data, performance of various forms of regression varies depending on the Bachelier correlation matrix, which is randomly re-generated on every run. Interestingly, we don't have this problem with the neural networks implemented in the original paper, where the performance of classical and differential networks is resilient to changes of model parameters. With polynomial regression, performance is variable, but, as the notebook demonstrates, differential regression  considerbaly improves the performance over classic and ridge regressions, every time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndQ6wgh_Rsec",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9v32SMlUsMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from scipy.stats import norm\n",
        "from scipy.optimize import minimize_scalar\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHKYl7b4Uy10",
        "colab_type": "text"
      },
      "source": [
        "# Bachelier pricer/simulator\n",
        "---\n",
        "Simulates a training set of basket payoff samples along with pathwise differentials and a test set of ground truth values, deltas and vegas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY63xJxzU9QC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper analytics\n",
        "def bachPrice(spot, strike, vol, T):\n",
        "    d = (spot - strike) / vol / np.sqrt(T)\n",
        "    return  vol * np.sqrt(T) * (d * norm.cdf(d) + norm.pdf(d))\n",
        "\n",
        "def bachDelta(spot, strike, vol, T):\n",
        "    d = (spot - strike) / vol / np.sqrt(T)\n",
        "    return norm.cdf(d)\n",
        "\n",
        "def bachVega(spot, strike, vol, T):\n",
        "    d = (spot - strike) / vol / np.sqrt(T)\n",
        "    return np.sqrt(T) * norm.pdf(d)\n",
        "#\n",
        "    \n",
        "# generates a random correlation matrix\n",
        "def genCorrel(n):\n",
        "    randoms = np.random.uniform(low=-1., high=1., size=(2*n, n))\n",
        "    cov = randoms.T @ randoms\n",
        "    invvols = np.diag(1. / np.sqrt(np.diagonal(cov)))\n",
        "    return np.linalg.multi_dot([invvols, cov, invvols])\n",
        "\n",
        "# main class\n",
        "class Bachelier:\n",
        "    \n",
        "    def __init__(self, \n",
        "                 n,\n",
        "                 T1=1, \n",
        "                 T2=2, \n",
        "                 K=1.10,\n",
        "                 volMult=1):\n",
        "        \n",
        "        self.n = n\n",
        "        self.T1 = T1\n",
        "        self.T2 = T2\n",
        "        self.K = K\n",
        "        self.volMult = volMult\n",
        "                \n",
        "    # training set: returns S1 (mxn), C2 (mx1) and dC2/dS1 (mxn)\n",
        "    def trainingSet(self, m, anti=True, seed=None, bktVol=0.2):\n",
        "    \n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # spots all currently 1, without loss of generality\n",
        "        self.S0 = np.repeat(1., self.n)\n",
        "        # random correl\n",
        "        self.corr = genCorrel(self.n)\n",
        "\n",
        "        # random weights\n",
        "        self.a = np.random.uniform(low=1., high=10., size=self.n)\n",
        "        self.a /= np.sum(self.a)\n",
        "        # random vols\n",
        "        vols = np.random.uniform(low=5., high = 50., size = self.n)\n",
        "        # normalize vols for a given volatility of basket, \n",
        "        # helps with charts without loss of generality\n",
        "        avols = (self.a * vols).reshape((-1,1))\n",
        "        v = np.sqrt(np.linalg.multi_dot([avols.T, self.corr, avols]).reshape(1))\n",
        "        self.vols = vols * bktVol / v\n",
        "        self.bktVol = bktVol\n",
        "\n",
        "        # Choleski etc. for simulation\n",
        "        diagv = np.diag(self.vols)\n",
        "        self.cov = np.linalg.multi_dot([diagv, self.corr, diagv])\n",
        "        self.chol = np.linalg.cholesky(self.cov) * np.sqrt(self.T2 - self.T1)\n",
        "        # increase vols for simulation of X so we have more samples in the wings\n",
        "        self.chol0 = self.chol * self.volMult * np.sqrt(self.T1 / (self.T2 - self.T1))\n",
        "        # simulations\n",
        "        normals = np.random.normal(size=[2, m, self.n])\n",
        "        inc0 = normals[0, :, :] @ self.chol0.T\n",
        "        inc1 = normals[1, :, :] @ self.chol.T\n",
        "    \n",
        "        S1 = self.S0 + inc0\n",
        "        \n",
        "        S2 = S1 + inc1\n",
        "        bkt2 = np.dot(S2, self.a)\n",
        "        pay = np.maximum(0, bkt2 - self.K)\n",
        "\n",
        "        # two antithetic paths\n",
        "        if anti:\n",
        "            \n",
        "            S2a = S1 - inc1\n",
        "            bkt2a = np.dot(S2a, self.a)\n",
        "            paya = np.maximum(0, bkt2a - self.K)\n",
        "            \n",
        "            X = S1\n",
        "            Y = 0.5 * (pay + paya)\n",
        "    \n",
        "            # differentials\n",
        "            Z1 =  np.where(bkt2 > self.K, 1.0, 0.0).reshape((-1,1)) * self.a.reshape((1,-1))\n",
        "            Z2 =  np.where(bkt2a > self.K, 1.0, 0.0).reshape((-1,1)) * self.a.reshape((1,-1))\n",
        "            Z = 0.5 * (Z1 + Z2)\n",
        "                    \n",
        "        # standard\n",
        "        else:\n",
        "        \n",
        "            X = S1\n",
        "            Y = pay\n",
        "            \n",
        "            # differentials\n",
        "            Z =  np.where(bkt2 > self.K, 1.0, 0.0).reshape((-1,1)) * self.a.reshape((1,-1))\n",
        "            \n",
        "        return X, Y.reshape((-1,1)), Z\n",
        "    \n",
        "    # test set: returns an array of independent, uniformly random spots \n",
        "    # with corresponding baskets, ground true prices, deltas and vegas\n",
        "    def testSet(self, lower=0.50, upper=1.50, num=4096, seed=None):\n",
        "        np.random.seed(seed)\n",
        "        spots = np.random.uniform(low=lower, high = upper, size=(num, self.n))\n",
        "        baskets = np.dot(spots, self.a).reshape((-1, 1))\n",
        "        prices = bachPrice(baskets, self.K, self.bktVol, self.T2 - self.T1)\n",
        "        deltas = bachDelta(baskets, self.K, self.bktVol, self.T2 - self.T1) @ self.a.reshape((1, -1))\n",
        "        vegas = bachVega(baskets, self.K, self.bktVol, self.T2 - self.T1) \n",
        "        return spots, baskets, prices.reshape((-1, 1)), deltas, vegas\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM-JBljJVnt1",
        "colab_type": "text"
      },
      "source": [
        "# Polynomial regressor\n",
        "---\n",
        "Implements SVD (actually, eigenvalue) based regressions formulas [found here](https://drive.google.com/file/d/1qoZB_p9DeK-LZPV76lTkx7esduwme4FT/view?usp=sharing) for standard, ridge and differential polynomial regression\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgivenZdWv71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PolyReg:\n",
        "\n",
        "  # compute all the polynomials we are going to need\n",
        "  def __init__(self, xTrain, yTrain, dydxTrain, validSize, xTest, yTest, dydxTest, maxDeg):\n",
        "    \n",
        "    # store\n",
        "\n",
        "    self.X = xTrain\n",
        "    self.Y = yTrain\n",
        "    self.dYdX = dydxTrain\n",
        "\n",
        "    self.tx = xTest\n",
        "    self.ty = yTest         \n",
        "    self.tdydx = dydxTest   \n",
        "\n",
        "    # split\n",
        "\n",
        "    # train set\n",
        "    self.ax = self.X[:-validSize, :]\n",
        "    self.ay = self.Y[:-validSize, :]\n",
        "    self.adydx = self.dYdX[:-validSize, :]\n",
        "\n",
        "    # valid set\n",
        "    self.vx = self.X[-validSize:, :]\n",
        "    self.vy = self.Y[-validSize:, :]\n",
        "    self.vdydx = self.dYdX[-validSize:, :]\n",
        "\n",
        "    # monomials\n",
        "\n",
        "    # generator, without constant\n",
        "    self.maxDeg = maxDeg\n",
        "    self.polGen = PolynomialFeatures(degree=maxDeg, include_bias=False)\n",
        "    \n",
        "    # train\n",
        "    self.aphi = self.polGen.fit_transform(self.ax)\n",
        "    # valid\n",
        "    self.vaphi = self.polGen.transform(self.vx)\n",
        "    # test\n",
        "    self.taphi = self.polGen.transform(self.tx)\n",
        "    \n",
        "    # powers, degrees\n",
        "    self.powers = self.polGen.powers_\n",
        "    self.degrees = np.sum(self.powers, axis=1)\n",
        "\n",
        "    # dphi: differentials of monomials to inputs, of shape (num examples x num monomials x num inputs)\n",
        "    # train\n",
        "    self.adphidx = self.aphi[:, :, np.newaxis] * self.powers[np.newaxis, :, :] / (self.ax[:, np.newaxis, :] + 1.0e-08)\n",
        "    # valid\n",
        "    self.vadphidx = self.vaphi[:, :, np.newaxis] * self.powers[np.newaxis, :, :] / (self.vx[:, np.newaxis, :] + 1.0e-08)\n",
        "    # test\n",
        "    self.tadphidx = self.taphi[:, :, np.newaxis] * self.powers[np.newaxis, :, :] / (self.tx[:, np.newaxis, :] + 1.0e-08)\n",
        "\n",
        "  # prepare regression of degree p on m examples\n",
        "  def prepare(self, p, m):\n",
        "    \n",
        "    # crop degrees\n",
        "    self.indices = np.argwhere(self.degrees <= p).reshape(-1)\n",
        "    \n",
        "    # crop and center data\n",
        "    # train phi\n",
        "    self.x = self.ax[:m, :]\n",
        "    phi = self.aphi[:m, self.indices]\n",
        "    self.muphi = phi.mean(axis=0).reshape(1, -1)\n",
        "    self.phi = phi - self.muphi\n",
        "    # train y\n",
        "    y = self.ay[:m, :]\n",
        "    self.muy = y.mean()\n",
        "    self.y = y - self.muy\n",
        "    # train derivs\n",
        "    self.dydx = self.adydx[:m, :]\n",
        "    self.dphidx = self.adphidx[:m, self.indices, :]\n",
        "    \n",
        "    # valid\n",
        "    self.vphi = self.vaphi[:, self.indices] - self.muphi\n",
        "    self.vdphidx = self.vadphidx[:, self.indices, :]\n",
        "\n",
        "    # test\n",
        "    self.tphi = self.taphi[:, self.indices] - self.muphi\n",
        "    self.tdphidx = self.tadphidx[:, self.indices, :]\n",
        "\n",
        "    # weight the train derivs for cost function\n",
        "    self.lamj = (y.var() / (self.dydx ** 2).mean(axis=0)).reshape(1, 1, -1)\n",
        "    self.dphidxw = self.dphidx * np.sqrt(self.lamj)\n",
        "\n",
        "  # regress\n",
        "  def regress(self, tikLambda = 0, eigCut = 1.0e-08):\n",
        "            \n",
        "      # eigenvalue regression \n",
        "      d, p = np.linalg.eigh(self.phi.T @ self.phi)\n",
        "      \n",
        "      # cut small eigenvalues\n",
        "      cut = eigCut * d.mean()\n",
        "      invD = np.where(d>cut, 1/(d + tikLambda ** 2), 0).reshape(1, -1)\n",
        "      \n",
        "      # result, see note\n",
        "      return ((p * invD) @ p.T @ self.phi.T @ self.y).reshape(-1, 1)    \n",
        "      \n",
        "  # find best tikhonov lambda by cross validation\n",
        "  def crossValidate(self):\n",
        "    \n",
        "    # objective function\n",
        "    def obj(tikLambda):\n",
        "      if tikLambda < 0:\n",
        "        return obj(0) * (1 + tikLambda ** 2)\n",
        "      beta = self.regress(tikLambda)\n",
        "      validErrors = self.validValues(beta) - self.vy\n",
        "      mse = np.mean(validErrors ** 2)\n",
        "      return mse \n",
        "    \n",
        "    # optimize\n",
        "    tikLambda = minimize_scalar(obj, tol=1.0e-02).x\n",
        "\n",
        "    # regress\n",
        "    return self.regress(tikLambda)\n",
        "\n",
        "  # differential regress, see memo\n",
        "  def diffRegress(self, diffLambda = 1, eigCut = 1.0e-08):\n",
        "\n",
        "    # sums over j of phij.T phij and phijT Zj\n",
        "    phiTphi = np.tensordot(self.dphidxw, self.dphidx, axes=([0,2],[0,2]))\n",
        "    phiTz = np.tensordot(self.dphidxw, self.dydx, axes=([0,2],[0,1])).reshape(-1,1)\n",
        "    \n",
        "    # eigenvalue regression\n",
        "    d, p = np.linalg.eigh(self.phi.T @ self.phi + diffLambda * phiTphi)\n",
        "  \n",
        "    # cut small eigenvalues\n",
        "    cut = eigCut * d.mean()\n",
        "    invD = np.where(d>cut, 1/d, 0).reshape(1, -1)\n",
        "  \n",
        "    # eigenvalue regression result\n",
        "    return ((p * invD) @ p.T @ (self.phi.T @ self.y + diffLambda * phiTz)).reshape(-1, 1)    \n",
        "              \n",
        "\n",
        "  # predict values and derivs\n",
        "  def predictValues(self, beta, phi):\n",
        "      return phi @ beta + self.muy\n",
        "  \n",
        "  def predictDerivs(self, beta, dphidx):\n",
        "      return np.tensordot(dphidx, beta, (1, 0)).reshape(dphidx.shape[0], -1)\n",
        "\n",
        "  # results on valid set, values\n",
        "  def validValues(self, beta):\n",
        "      return self.predictValues(beta, self.vphi)\n",
        "\n",
        "  # results on test set, values\n",
        "  def testValues(self, beta):\n",
        "      return self.predictValues(beta, self.tphi)\n",
        "    \n",
        "  # results on test set, deltas\n",
        "  def testDerivs(self, beta):\n",
        "      return self.predictDerivs(beta, self.tdphidx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcjvSL9Hcpup",
        "colab_type": "text"
      },
      "source": [
        "# Testing Functions\n",
        "---\n",
        "Perform multiple regressions in many different configurations at once, and show results in charts\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9q7GiU4qEuh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(dim, maxNtrain, simulSeed, nTest, testSeed, validSize, degs, sizes , deltidx=0, doTikhonov=True):\n",
        "\n",
        "    # simulation\n",
        "    print(\"simulating training, valid and test sets\")\n",
        "    bach = Bachelier(dim)\n",
        "    xTrain, yTrain, dydxTrain = bach.trainingSet(maxNtrain, seed=simulSeed)\n",
        "    xTest, xAxis, yTest, dydxTest, vegas = bach.testSet(num=nTest, seed=testSeed)\n",
        "    print(\"done\")\n",
        "\n",
        "    # regressor\n",
        "    print(\"computing monomials\")\n",
        "    regressor = PolyReg(xTrain, yTrain, dydxTrain, validSize, xTest, yTest, dydxTest, max(degs))\n",
        "    print(\"done\")\n",
        "    \n",
        "    predvalues = {}    \n",
        "    preddeltas = {}\n",
        "    for size in sizes:        \n",
        "        for deg in degs:\n",
        "            \n",
        "            print(\"doing size %d, degree %d\" % (size, deg))\n",
        "            print(\"prep regressor\")\n",
        "            regressor.prepare(deg, size)\n",
        "            print(\"done\")\n",
        "                \n",
        "            print(\"standard regression\")\n",
        "            beta = regressor.regress()\n",
        "            predictions = regressor.testValues(beta)            \n",
        "            deltas = regressor.testDerivs(beta)\n",
        "            predvalues[(\"standard\", size, deg)] = predictions\n",
        "            preddeltas[(\"standard\", size, deg)] = deltas[:, deltidx]\n",
        "            print(\"done\")            \n",
        "            \n",
        "            print(\"differential regression\")\n",
        "            beta2 = regressor.diffRegress()\n",
        "            predictions2 = regressor.testValues(beta2)\n",
        "            deltas2 = regressor.testDerivs(beta2)\n",
        "            predvalues[(\"differential\", size, deg)] = predictions2\n",
        "            preddeltas[(\"differential\", size, deg)] = deltas2[:, deltidx]\n",
        "            print(\"done\")            \n",
        "            \n",
        "            if doTikhonov:\n",
        "                print(\"ridge regression by cross validation\")\n",
        "                beta3 = regressor.crossValidate()\n",
        "                predictions3 = regressor.testValues(beta3)\n",
        "                deltas3 = regressor.testDerivs(beta3)\n",
        "                print(\"done\")            \n",
        "\n",
        "                predvalues[(\"tikhonov\", size, deg)] = predictions3\n",
        "                preddeltas[(\"tikhonov\", size, deg)] = deltas3[:, deltidx]\n",
        "            \n",
        "\n",
        "\n",
        "    return xAxis, yTest, dydxTest[:, deltidx], vegas, predvalues, preddeltas\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-2tVxvhqYNK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def graph(title, predictions, xAxis, xAxisName, yAxisName, targets, regType, degs, sizes, computeRmse=False, weights=None):\n",
        "    \n",
        "    numRows = len(sizes)\n",
        "    numCols = len(degs)\n",
        "\n",
        "    fig, ax = plt.subplots(numRows, numCols, squeeze=False)\n",
        "    fig.set_size_inches(4 * numCols + 1.5, 4 * numRows)\n",
        "\n",
        "    for i, size in enumerate(sizes):\n",
        "        ax[i,0].annotate(\"size %d\" % size, xy=(0, 0.5), xytext=(-ax[i,0].yaxis.labelpad-5, 0),\n",
        "          xycoords=ax[i,0].yaxis.label, textcoords='offset points',\n",
        "          ha='right', va='center')\n",
        "  \n",
        "    for j, deg in enumerate(degs):\n",
        "        ax[0,j].set_title(\"degree %d\" % deg)\n",
        "  \n",
        "    for i, size in enumerate(sizes):        \n",
        "        for j, deg in enumerate(degs):\n",
        "\n",
        "            if computeRmse:\n",
        "                errors = predictions[(regType, size, deg)] - targets\n",
        "                if weights is not None:\n",
        "                    errors /= weights\n",
        "                rmse = np.sqrt((errors ** 2).mean(axis=0))\n",
        "                t = \"rmse %.1f vegas\" % rmse\n",
        "            else:\n",
        "                t = xAxisName\n",
        "                \n",
        "            ax[i,j].set_xlabel(t)            \n",
        "            ax[i,j].set_ylabel(yAxisName)\n",
        "\n",
        "            ax[i,j].plot(xAxis*100, predictions[(regType, size, deg)]*100, 'co', markersize=2, markerfacecolor='white', label=\"predicted\")\n",
        "            ax[i,j].plot(xAxis*100, targets*100, 'r.', markersize=0.5, label='targets')\n",
        "\n",
        "            ax[i,j].legend(prop={'size': 8}, loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.94)\n",
        "    plt.suptitle(\"% s -- %s in %s regression\" % (title, yAxisName, regType), fontsize=16)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rePjxmanTTQ2",
        "colab_type": "text"
      },
      "source": [
        "# Parameters\n",
        "---\n",
        "Input the parameters of your desired configurations here\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z13f_oozTZ_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dimension of the basket / Bachelier model -- note maximum regression can handle is 7 dimension with degree 5\n",
        "basketDim = 7   \n",
        "\n",
        "# polynomial degrees and simulation set sizes to perform\n",
        "degs = [3,5] \n",
        "sizes = [1024, 8192, 32768]\n",
        "validSize=8192 # for tikhonov cross-validation\n",
        "\n",
        "# Tikhonov/Ridge is slow with cross-validation\n",
        "# set to False to perform all regression in seconds\n",
        "# as opposed to up to a minute \n",
        "doTikhonov = True \n",
        "showDeltas = True\n",
        "deltidx = 0   # which delta to show?\n",
        "\n",
        "# no not edit\n",
        "maxNtrain = max(sizes) + (validSize if doTikhonov else 0)\n",
        "\n",
        "# seed\n",
        "simulSeed = np.random.randint(0, 10000) \n",
        "print(\"using seed %d\" % simulSeed)\n",
        "testSeed = None\n",
        "\n",
        "# number of test scenarios\n",
        "nTest = 4096"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0WzS1thUG9N",
        "colab_type": "text"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0AT9ikwUDrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# performs all requested regressions and collects results\n",
        "\n",
        "xAxis, yTest, dydxTest, vegas, values, deltas = \\\n",
        "  test(basketDim, maxNtrain, simulSeed, nTest, testSeed, validSize, degs, sizes, deltidx, doTikhonov)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFDpvlEeUSRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# standard regression value chart\n",
        "graph(\"dimension %d\" % basketDim, values, xAxis, \"\", \"values\", yTest, \"standard\", degs, sizes, computeRmse=True, weights=vegas/100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD7TU6hTUhqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# standard regression delta chart\n",
        "if showDeltas:\n",
        "    graph(\"dimension %d\" % basketDim, deltas, xAxis, \"basket\", \"deltas\", dydxTest, \"standard\", degs, sizes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC-HKwSwUrkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# differential regression value chart\n",
        "graph(\"dimension %d\" % basketDim, values, xAxis, \"\", \"values\", yTest, \"differential\", degs, sizes, computeRmse=True, weights=vegas/100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUAFc1BhUy-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# differential regression delta chart\n",
        "graph(\"dimension %d\" % basketDim, deltas, xAxis, \"basket\", \"deltas\", dydxTest, \"differential\", degs, sizes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaXEzjCwU6SW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ridge regression value charts\n",
        "if doTikhonov:\n",
        "    graph(\"dimension %d\" % basketDim, values, xAxis, \"\", \"values\", yTest, \"tikhonov\", degs, sizes, computeRmse=True, weights=vegas/100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0A7RwDgVD0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ridge regression delta charts\n",
        "if doTikhonov and showDeltas:\n",
        "        graph(\"dimension %d\" % basketDim, deltas, xAxis, \"basket\", \"deltas\", dydxTest, \"tikhonov\", degs, sizes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwyLl-evXr86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
